{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import time\n",
    "\n",
    "def convert_Date(italian_date):\n",
    "    # Mapping of Italian month names to month numbers\n",
    "    italianMonths_to_Number = {\n",
    "        \"gennaio\": \"01\",\n",
    "        \"febbraio\": \"02\",\n",
    "        \"marzo\": \"03\",\n",
    "        \"aprile\": \"04\",\n",
    "        \"maggio\": \"05\",\n",
    "        \"giugno\": \"06\",\n",
    "        \"luglio\": \"07\",\n",
    "        \"agosto\": \"08\",\n",
    "        \"settembre\": \"09\",\n",
    "        \"ottobre\": \"10\",\n",
    "        \"novembre\": \"11\",\n",
    "        \"dicembre\": \"12\",\n",
    "    }\n",
    "\n",
    "    # Split the Italian date into parts\n",
    "    date_parts = italian_date.split()\n",
    "\n",
    "    # Get day, month, and year\n",
    "    day = date_parts[0]\n",
    "    month = italianMonths_to_Number[date_parts[1]]\n",
    "    year = date_parts[2]\n",
    "\n",
    "    # Format the date in the desired format\n",
    "    formatted_date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "    return formatted_date\n",
    "\n",
    "# Function to generate a CSV file with the extracted links\n",
    "def create_csv(keyword, from_date, to_date, urls_list):\n",
    "    # Define the output directory one level up from the current directory\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), os.pardir, \"PreprocessedOutput\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # Format the keyword to name the file\n",
    "    filename = keyword.replace(\" \", \"_\")\n",
    "\n",
    "    # Build the CSV file path/name with dates\n",
    "    csv_file_path = os.path.join(\n",
    "        OUTPUT_DIR, \"_\".join([filename, from_date, to_date]) + \".csv\"\n",
    "    )\n",
    "\n",
    "    # Create a list of dictionaries from URLs with the header \"URL\"\n",
    "    data = [{\"URL\": url} for url in urls_list]\n",
    "\n",
    "    if os.path.exists(csv_file_path):\n",
    "        # The file exists, open in read mode to review content\n",
    "        with open(csv_file_path, mode=\"r\", newline=\"\") as file:\n",
    "            existing_data = list(csv.DictReader(file))\n",
    "\n",
    "        # Check if the new data is already in the existing file\n",
    "        existing_urls = set(row[\"URL\"] for row in existing_data)\n",
    "        new_urls = set(row[\"URL\"] for row in data)\n",
    "        if not new_urls.issubset(existing_urls):\n",
    "            # The file exists, open in append mode\n",
    "            with open(csv_file_path, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "\n",
    "                # Add new URL data rows to the existing file\n",
    "                for row in data:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        # print(f\"Data added to existing file: {csv_file_path}\")\n",
    "    else:\n",
    "        # The file does not exist, open in write mode\n",
    "        with open(csv_file_path, mode=\"w\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write URL data rows\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        print(f\"New CSV file created: {csv_file_path}\")\n",
    "\n",
    "    return csv_file_path\n",
    "\n",
    "# Extracts the total number of pages from the search results\n",
    "def get_the_total_number_of_pages(url):\n",
    "    try:\n",
    "        # Perform a GET request and parse HTML\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # Check the status code\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            root = html.fromstring(str(soup))\n",
    "\n",
    "            # XPath to extract the total pages text\n",
    "            # Handle case where there are no results\n",
    "            try:\n",
    "                paragraph_text = root.xpath(\n",
    "                    '//*[@id=\"lista-risultati\"]/div/p/text()[2]'\n",
    "                )[0]\n",
    "                total_pages = paragraph_text.split()[1]\n",
    "            except:\n",
    "                total_pages = 0\n",
    "\n",
    "            # Convert to integer and return\n",
    "            return int(total_pages)\n",
    "\n",
    "        elif page.status_code == 403:\n",
    "            print(\n",
    "                \"403 Forbidden status code encountered. Waiting 3 minutes before retrying...\"\n",
    "            )\n",
    "\n",
    "            for i in range(3):\n",
    "                print(f\"Waiting {i+1} minute...\")\n",
    "                time.sleep(60)\n",
    "\n",
    "            # After waiting, retry to get the total number of pages\n",
    "            return get_the_total_number_of_pages(url)\n",
    "        else:\n",
    "            print(f\"Unexpected status code: {page.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Gets the date of the most recent article result\n",
    "def get_date_of_next_period(url):\n",
    "    try:\n",
    "        # Request page and parse HTML\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # Check the status code\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            root = html.fromstring(str(soup))\n",
    "\n",
    "            # XPath to extract the last date in the results set\n",
    "            dates = root.xpath('//*[@id=\"lista-risultati\"]/article/aside/a')[-1]\n",
    "            date = dates.text_content().strip()\n",
    "\n",
    "            # Return cleaned date string\n",
    "            return date\n",
    "        elif page.status_code == 403:\n",
    "            print(\n",
    "                \"403 Forbidden status code encountered. Waiting 3 minutes before retrying...\"\n",
    "            )\n",
    "\n",
    "            for i in range(3):\n",
    "                print(f\"Waiting {i+1} minute...\")\n",
    "                time.sleep(60)\n",
    "\n",
    "            # After waiting, retry to get the date of the next period\n",
    "            return get_date_of_next_period(url)\n",
    "        else:\n",
    "            print(f\"Unexpected status code: {page.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generates a list of URLs for search result pages\n",
    "def urls_generator(keyword, from_date, to_date, modality, number_of_pages):\n",
    "    # Format the keyword string for URL\n",
    "    keyword = keyword.replace(\" \", \"+\")\n",
    "\n",
    "    # Base URL for search results with placeholder parameters\n",
    "    #! Note: Please do not modify this construction as it will alter the desired behavior or result in a fatal error.\n",
    "    url_base = \"https://ricerca.repubblica.it/ricerca/repubblica?query={}&fromdate={}&todate={}&sortby=adate&author=&mode={}&page={}\"\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    # Iterate through the number of pages\n",
    "    for page in range(1, number_of_pages + 1):\n",
    "        # Populate base URL with parameters\n",
    "        # The page number is iterated\n",
    "        url = url_base.format(keyword, from_date, to_date, modality, page)\n",
    "\n",
    "        # Add to the list of URLs\n",
    "        urls.append(url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "# Main scraping function\n",
    "def scraper_one(keyword, from_date, to_date, modality):\n",
    "    # Generate the initial set of search result URLs\n",
    "    urls = urls_generator(keyword, from_date, to_date, modality, 1)\n",
    "\n",
    "    # Get the total number of pages for the search\n",
    "    total_number_of_pages = get_the_total_number_of_pages(urls[0])  # Example 140\n",
    "\n",
    "    # Handle no results case\n",
    "    if total_number_of_pages == 0:\n",
    "        return print(\"No news found with the provided entry\")\n",
    "\n",
    "    # If < 50 pages, scrape all\n",
    "    elif total_number_of_pages <= 50:\n",
    "        urls_list = urls_generator(\n",
    "            keyword, from_date, to_date, modality, total_number_of_pages\n",
    "        )\n",
    "\n",
    "        # Save URLs to CSV\n",
    "        name_output = create_csv(keyword, from_date, to_date, urls_list)\n",
    "\n",
    "        return name_output\n",
    "\n",
    "    # If > 50 pages, scrape in batches\n",
    "    else:\n",
    "        max_pages_per_keyword = 50\n",
    "\n",
    "        # First batch of 50 URLs\n",
    "        urls_list = urls_generator(\n",
    "            keyword, from_date, to_date, modality, max_pages_per_keyword\n",
    "        )\n",
    "\n",
    "        # Save to CSV (create)\n",
    "        name_output = create_csv(keyword, from_date, to_date, urls_list)\n",
    "\n",
    "        # Scrape in batches of 50 using date pagination\n",
    "        while True:\n",
    "            # Get the start date of the next period\n",
    "            date_of_next_period_in_italian = get_date_of_next_period(urls_list[-1])\n",
    "            date_of_next_period = convert_Date(date_of_next_period_in_italian)\n",
    "            print(\"step01_URLgenerAtor, processing in \" + date_of_next_period)\n",
    "\n",
    "            # Generate a single URL to get the total number of pages\n",
    "            urls_list = urls_generator(\n",
    "                keyword, date_of_next_period, to_date, modality, 1\n",
    "            )  # returns array\n",
    "\n",
    "            # Get the total number of pages for the new date range\n",
    "            total_number_of_pages = get_the_total_number_of_pages(urls_list[0])\n",
    "\n",
    "            # Handle total pages cases\n",
    "            if total_number_of_pages > 50:\n",
    "                # If more than 50 pages, generate batch\n",
    "                urls_list = urls_generator(\n",
    "                    keyword,\n",
    "                    date_of_next_period,\n",
    "                    to_date,\n",
    "                    modality,\n",
    "                    max_pages_per_keyword,\n",
    "                )\n",
    "\n",
    "                # Save to CSV (append)\n",
    "                create_csv(keyword, from_date, to_date, urls_list)\n",
    "\n",
    "            elif total_number_of_pages <= 50:\n",
    "                # If less than 50, generate all pages\n",
    "                urls_list = urls_generator(\n",
    "                    keyword,\n",
    "                    date_of_next_period,\n",
    "                    to_date,\n",
    "                    modality,\n",
    "                    total_number_of_pages,\n",
    "                )\n",
    "\n",
    "                # Save to CSV (append)\n",
    "                create_csv(keyword, from_date, to_date, urls_list)\n",
    "\n",
    "                print(\n",
    "                    \"All existing URLs for the keyword have been saved\"\n",
    "                )\n",
    "\n",
    "                break\n",
    "        print('**********Complete url generator***********')\n",
    "        # Returns the file name for the next process\n",
    "        return name_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file created: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "Processing in 2023-02-02\n",
      "Data added to existing file: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "Processing in 2023-03-24\n",
      "Data added to existing file: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "Processing in 2023-05-18\n",
      "Data added to existing file: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "Processing in 2023-07-09\n",
      "Data added to existing file: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "Processing in 2023-09-04\n",
      "Data added to existing file: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "Processing in 2023-11-28\n",
      "Data added to existing file: d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n",
      "All existing URLs for the keyword have been saved\n",
      "d:\\Dropbox\\Anzony\\repubblica\\LaRepubblicaDataHarvest\\src\\..\\PreprocessedOutput\\mafia_nigeriana_2023-01-01_2024-01-01.csv\n"
     ]
    }
   ],
   "source": [
    "# if __name__== \"__main__\":\n",
    "#     file_process_file = scraper_one(\"mafia nigeriana\", '2023-01-01', \"2024-01-01\", \"any\")\n",
    "#     print(file_process_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvlarepublica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
