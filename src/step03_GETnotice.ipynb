{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  # For HTML analysis\n",
    "import requests  # To make HTTP requests\n",
    "import csv  # To read and write CSV files\n",
    "import os  # For file system operations like checking paths\n",
    "import time  # For pauses between requests\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re  # For regular expression operations\n",
    "\n",
    "\n",
    "# Modifying the URL to access the news, we save the new links\n",
    "# the new file must end in: _approved_pattern.csv\n",
    "\n",
    "def separate_by_pattern(file_path):\n",
    "\n",
    "    url_pattern = \"https://quotidiano.repubblica.it/edicola/searchdetail\\?id=http://archivio.repubblica.extra.kataweb.it/archivio/repubblica/\"\n",
    "\n",
    "    # Pattern to follow\n",
    "    new_url_base = \"https://ricerca.repubblica.it/repubblica/archivio/repubblica/\"\n",
    "\n",
    "    # Create file name that contains urls with the pattern\n",
    "    matched_urls_csv = os.path.splitext(file_path)[0] + \"_Matched_URLs.csv\"\n",
    "\n",
    "    # Create file name that contains urls without the pattern\n",
    "    unmatched_urls_csv = os.path.splitext(file_path)[0] + \"_Unmatched_URLs.csv\"\n",
    "\n",
    "    if not os.path.exists(matched_urls_csv):\n",
    "\n",
    "        # Read the CSV file\n",
    "        df_src = pd.read_csv(file_path)\n",
    "\n",
    "        # Filter rows that do not contain the pattern in the 'link' column\n",
    "        rows_without_pattern = df_src[~df_src[\"link\"].str.contains(url_pattern, na=False)]\n",
    "\n",
    "        if len(rows_without_pattern) > 0:\n",
    "            rows_without_pattern.to_csv(unmatched_urls_csv, index=False)\n",
    "            print(f'The file {os.path.basename(unmatched_urls_csv)} was created successfully')\n",
    "            print(f\"Check in: {unmatched_urls_csv}\")\n",
    "        else:\n",
    "            rows_without_pattern = None\n",
    "\n",
    "        # Filter rows that contain the pattern in the 'link' column\n",
    "        rows_with_pattern = df_src[df_src[\"link\"].str.contains(url_pattern, na=False)]\n",
    "\n",
    "        # Generate new column\n",
    "        matched = []\n",
    "        for link in rows_with_pattern['link']:\n",
    "\n",
    "            # Remove pattern and replace base\n",
    "            notice_link = re.sub(url_pattern, new_url_base, link)\n",
    "\n",
    "            # Add to the list as a dictionary\n",
    "            matched.append({\"link\": notice_link})\n",
    "\n",
    "        df_final = pd.DataFrame(matched)\n",
    "\n",
    "        # Save the filtered rows in a new CSV file\n",
    "        df_final.to_csv(matched_urls_csv, index=False)\n",
    "\n",
    "        # Print the name of the output file\n",
    "        print(f'The file {os.path.basename(matched_urls_csv)} was created successfully')\n",
    "        print(f\"Check in: {matched_urls_csv}\")\n",
    "\n",
    "    else:\n",
    "        # Print the number of rows found\n",
    "        print(f\"The file {os.path.basename(matched_urls_csv)} already exists.\")\n",
    "        print(f\"Check in: {matched_urls_csv}\")\n",
    "\n",
    "    return matched_urls_csv, unmatched_urls_csv\n",
    "\n",
    "\n",
    "def parse(response, src_filename):\n",
    "    # *** Analyze HTML content ***\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    try:\n",
    "        article = soup.find(\"article\")\n",
    "        title_h1 = article.find(\"h1\")\n",
    "        if title_h1 is not None:\n",
    "            title = title_h1.get_text(separator=\" \", strip=True) or 'No title'\n",
    "    except:\n",
    "        title = \"No title\"\n",
    "    try:\n",
    "        article = soup.find(\"article\")\n",
    "        content_tag = article.find(\"p\")\n",
    "\n",
    "        if content_tag is not None:\n",
    "            content = content_tag.get_text(separator=\" \", strip=True)\n",
    "        else:\n",
    "            content = \"\"\n",
    "    except:\n",
    "        content = \"\"\n",
    "\n",
    "    date = (\n",
    "        article.find(\"aside\").find(\"a\").find(\"time\").get_text(separator=\" \", strip=True)\n",
    "    )\n",
    "    # *** End of analysis ***\n",
    "\n",
    "    # Target data\n",
    "    article_data = [\n",
    "        {\"title\": title, \"content\": content, \"date\": date, \"src_url\": response.url}\n",
    "    ]\n",
    "\n",
    "    # Define field names\n",
    "    fieldnames = [\"title\", \"content\", \"date\", \"src_url\"]\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    OUTPUT_DIR = os.path.join(os.getcwd(), os.pardir, \"Output\")\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    # File path for logging visited links\n",
    "    basename = os.path.basename(src_filename).split(\"_\")[0] + \"_output_notice.csv\"\n",
    "    output = os.path.join(OUTPUT_DIR, basename)\n",
    "\n",
    "    # Check if the output file exists\n",
    "    mode = \"a\" if os.path.exists(output) else \"w\"\n",
    "    # If the file exists, open in append mode, otherwise create (write mode)\n",
    "\n",
    "    with open(output, mode, newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "\n",
    "        if mode == \"w\":\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerows(article_data)\n",
    "\n",
    "def scraper(filename):\n",
    "    # Files filtered by pattern\n",
    "    matchedURL_csv, unmatchedURL_csv = separate_by_pattern(filename)\n",
    "\n",
    "    # File path for logging visited links\n",
    "    dirname = os.path.dirname(filename)\n",
    "    basename = os.path.basename(filename).split(\"_\")[0] + \"_scraper_activity_record.csv\"\n",
    "    records_filepath = os.path.join(dirname, basename)\n",
    "\n",
    "    if not os.path.exists(records_filepath):\n",
    "        with open(records_filepath, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=['notice_link'])\n",
    "            writer.writeheader()\n",
    "\n",
    "    # Backup\n",
    "    df_records = pd.read_csv(records_filepath)\n",
    "\n",
    "    # Read the CSV file containing the links that matched the pattern\n",
    "    df = pd.read_csv(matchedURL_csv)\n",
    "\n",
    "    urls_to_skip = set(df_records.notice_link)\n",
    "\n",
    "    # Iterate over each row of data\n",
    "    for iteration, row in df.iterrows():\n",
    "        link = row[\"link\"]  # Get the link from the 'link' column\n",
    "\n",
    "        if link not in urls_to_skip:\n",
    "            print(f'step03_GETnotice, iteration on row # {iteration}')\n",
    "\n",
    "            # Scrape with BeautifulSoup\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "\n",
    "                # Extract data from the website here\n",
    "                parse(response, matchedURL_csv)\n",
    "\n",
    "                # Check if the file exists\n",
    "                mode = \"a\" if os.path.exists(records_filepath) else \"w\"\n",
    "                # If the file exists, open in append mode\n",
    "                with open(records_filepath, 'a', newline=\"\", encoding=\"utf-8\") as file:\n",
    "                    writer = csv.DictWriter(file, fieldnames=['notice_link'], extrasaction=\"ignore\")\n",
    "                    if mode == \"w\":\n",
    "                        writer.writeheader()\n",
    "                    # Write the news and its metadata in the file\n",
    "                    writer.writerow({\"notice_link\": link})\n",
    "\n",
    "            elif response.status_code == 403:\n",
    "                print('The status is 403\\n' * 3)\n",
    "                print('Starting the 4-minute wait')\n",
    "\n",
    "                # Wait 4 minutes before retrying\n",
    "                for i in range(1, 5):\n",
    "                    time.sleep(60)\n",
    "                    print(f'{i} minutes have passed')\n",
    "                print('Restarting the scraper based on the backup')\n",
    "        else:\n",
    "            print(f\"step03_GETnotice Row # {iteration} Link already processed, skipping...\")\n",
    "\n",
    "    print('Scraping complete')\n",
    "    if unmatchedURL_csv is not None:\n",
    "        print(f'Urls that did not match the pattern are in {unmatchedURL_csv}')\n",
    "        print('Starting step04_GETnotice_unmatchedURLs')\n",
    "    return unmatchedURL_csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvlarepublica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
