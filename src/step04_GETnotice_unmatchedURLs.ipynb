{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_6368\\470457417.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd  # For data manipulation and analysis\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import requests  # For making HTTP requests\n",
    "import csv  # For reading and writing CSV files\n",
    "import os  # For file system operations like checking paths\n",
    "import time  # For pauses between requests\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from multiprocessing import Pool  # For parallelizing tasks\n",
    "from functools import partial\n",
    "\n",
    "def parse_unmadchedURLs(response, title_link, date_link, src_url):\n",
    "\n",
    "    # *** Parse HTML content ***\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        container_a = soup.find(\"div\", id=\"article-body\")\n",
    "        content_a = container_a.get_text(separator=\" \", strip=True)\n",
    "    except:\n",
    "        content_a = \"article-body not found\"\n",
    "\n",
    "    try:\n",
    "        container_b = soup.find(\"div\", id=\"story__summary\")\n",
    "        content_b = container_b.get_text(separator=\" \", strip=True)\n",
    "    except:\n",
    "        content_b = \"story__summary not found\"\n",
    "\n",
    "    try:\n",
    "        container_c = soup.find(\"div\", id=\"detail_summary\")\n",
    "        content_c = container_c.get_text(separator=\" \", strip=True)\n",
    "    except:\n",
    "        content_c = \"detail_summary not found\"\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "    # *** End of parsing ***\n",
    "\n",
    "    # Target data\n",
    "    article_data = [\n",
    "        {\n",
    "            \"title\": title_link,\n",
    "            \"article_body\": content_a,\n",
    "            \"story__summary\": content_b,\n",
    "            \"detail_summary\": content_c,\n",
    "            \"date\": date_link,\n",
    "            \"link\": response.url,\n",
    "            \"src_url\": src_url,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return article_data\n",
    "\n",
    "def check_mode(file_name):\n",
    "    mode = \"a\" if os.path.exists(file_name) else \"w\"\n",
    "    return mode\n",
    "\n",
    "def write_csv(file_name, mode, fieldnames, article_data=None):\n",
    "\n",
    "    with open(file_name, mode, newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(\n",
    "            file, fieldnames=fieldnames, extrasaction=\"ignore\"\n",
    "        )\n",
    "        # If opened in write mode (new file), write headers\n",
    "        if mode == \"w\":\n",
    "            writer.writeheader()\n",
    "        # Write article data to CSV file\n",
    "        if article_data:\n",
    "            writer.writerows(article_data)\n",
    "\n",
    "def scraper_unmatchedURLS(row, output_filename):\n",
    "\n",
    "    # File path for visited links log.\n",
    "    dirname_rec = os.path.dirname(output_filename)\n",
    "    basename_rec = os.path.basename(output_filename).split(\"_\")[0] + \"_activity_rec_unmatchedURLs.csv\"\n",
    "    # Get the absolute path depending on the operating system.\n",
    "    activty_records_path = os.path.join(dirname_rec, basename_rec)\n",
    "\n",
    "    mode_b = check_mode(activty_records_path)\n",
    "\n",
    "    # Add to the scraping log\n",
    "    write_csv(activty_records_path, mode_b, [\"link\"])\n",
    "\n",
    "    # Read the scraping log\n",
    "    df_backup = pd.read_csv(activty_records_path)\n",
    "\n",
    "    # Read the scraping log skipping duplicates\n",
    "    urls_to_skip = set(df_backup.link)\n",
    "\n",
    "    link = row[\"link\"]  # Gets the link from the 'link' column\n",
    "    title_link = row[\"title\"]  # Gets the text from the 'title' column\n",
    "    date_link = row[\"date\"]  # Gets the text from the 'date' column\n",
    "    src_url = row[\"page_url\"]  # Gets the text from the 'page_url' column\n",
    "\n",
    "    if link not in urls_to_skip:\n",
    "        try:\n",
    "            # Crawling with BeautifulSoup\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "\n",
    "\n",
    "                # Extract data from the website here\n",
    "                article_data = parse_unmadchedURLs(\n",
    "                    response, title_link, date_link, src_url\n",
    "                )\n",
    "\n",
    "                # Define field names\n",
    "                fieldnames = [\n",
    "                    \"title\",\n",
    "                    \"article_body\",\n",
    "                    \"story__summary\",\n",
    "                    \"detail_summary\",\n",
    "                    \"date\",\n",
    "                    \"link\",\n",
    "                    \"src_url\",\n",
    "                ]\n",
    "\n",
    "                # Crear directorio de salida si no existe\n",
    "                OUTPUT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"Output\"))\n",
    "                if not os.path.exists(OUTPUT_DIR):\n",
    "                    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "                # Nombre del archivo de salida\n",
    "\n",
    "                basename_output = os.path.basename(output_filename).split(\"_\")[0] + \"_output_notice_unmatchedURLs.csv\"\n",
    "                name_output = os.path.join(OUTPUT_DIR, basename_output)\n",
    "\n",
    "                # Check if output file exists\n",
    "                mode_a = check_mode(name_output)\n",
    "\n",
    "                # If the file exists, open in append mode, otherwise create (write mode)\n",
    "                write_csv(name_output, mode_a, fieldnames, article_data)\n",
    "\n",
    "                links_backup = {\"link\": link}\n",
    "\n",
    "                # Check if the file exists\n",
    "                mode_b = check_mode(activty_records_path)\n",
    "\n",
    "                # If the file exists, open in append mode\n",
    "                write_csv(activty_records_path, mode_b, ['link'], [links_backup])\n",
    "\n",
    "\n",
    "            elif response.status_code == 403:\n",
    "\n",
    "                print(\"Status is 403\")\n",
    "\n",
    "                print(\"Starting 3-minute wait\")\n",
    "\n",
    "                # Wait 3 minutes before retrying\n",
    "                for i in range(1, 4):\n",
    "                    time.sleep(60)\n",
    "                    print(f\"{i} minutes have passed\")\n",
    "                print(\"Restarting the scraper based on backup\")\n",
    "\n",
    "            elif response.status_code == 307:\n",
    "                print(link)\n",
    "                print(\"Too many redirects\")\n",
    "                pass  # Continue with the next iteration of the loop\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Check: \\n{link}')\n",
    "            print(f\"Error: {e}\")\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        print(f\"Row already processed, skipping...{activty_records_path}\")\n",
    "\n",
    "    print(\"step04_GETnotice_unmatchedURLs, Crawling complete\")\n",
    "\n",
    "def main_unmatched_URLs(input_csv):\n",
    "\n",
    "    if os.path.exists(input_csv):\n",
    "\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Send the path\n",
    "        parital_scraper = partial(scraper_unmatchedURLS, output_filename = input_csv)\n",
    "\n",
    "        with Pool() as pool:\n",
    "            pool.map(parital_scraper, df.to_dict(\"records\"))\n",
    "    else:\n",
    "        print(\"Input CSV file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main('d:\\\\Dropbox\\\\Anzony\\\\repubblica\\\\LaRepubblicaDataHarvest_Es\\\\PreprocessedOutput\\\\mafia_urls_processed_Unmatched_URLs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_unmadchedURLs(response, title_link, date_link, src_url):\n",
    "    # *** Parse HTML content ***\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        container_a = soup.find(\"div\", id=\"article-body\")\n",
    "        content_a = container_a.get_text(separator=\" \", strip=True)\n",
    "    except:\n",
    "        content_a = \"article-body not found\"\n",
    "\n",
    "    try:\n",
    "        container_b = soup.find(\"div\", id=\"story__summary\")\n",
    "        content_b = container_b.get_text(separator=\" \", strip=True)\n",
    "    except:\n",
    "        content_b = \"story__summary not found\"\n",
    "\n",
    "    try:\n",
    "        container_c = soup.find(\"div\", id=\"detail_summary\")\n",
    "        content_c = container_c.get_text(separator=\" \", strip=True)\n",
    "    except:\n",
    "        content_c = \"detail_summary not found\"\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "    # *** End of parsing ***\n",
    "\n",
    "    # Target data\n",
    "    article_data = [\n",
    "        {\n",
    "            \"title\": title_link,\n",
    "            \"article_body\": content_a,\n",
    "            \"story__summary\": content_b,\n",
    "            \"detail_summary\": content_c,\n",
    "            \"date\": date_link,\n",
    "            \"link\": response.url,\n",
    "            \"src_url\": src_url,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return article_data\n",
    "\n",
    "def check_mode(file_name):\n",
    "    mode = \"a\" if os.path.exists(file_name) else \"w\"\n",
    "    return mode\n",
    "\n",
    "def write_csv(file_name, mode, fieldnames, article_data=None):\n",
    "    with open(file_name, mode, newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        # If opened in write mode (new file), write headers\n",
    "        if mode == \"w\":\n",
    "            writer.writeheader()\n",
    "        # Write article data to CSV file\n",
    "        if article_data:\n",
    "            writer.writerows(article_data)\n",
    "\n",
    "def scraper_unmatchedURLS(row, output_filename):\n",
    "    # File path for visited links log.\n",
    "    dirname_rec = os.path.dirname(output_filename)\n",
    "    basename_rec = os.path.basename(output_filename).split(\"_\")[0] + \"_activity_rec_unmatchedURLs.csv\"\n",
    "    # Get the absolute path depending on the operating system.\n",
    "    activty_records_path = os.path.join(dirname_rec, basename_rec)\n",
    "\n",
    "    mode_b = check_mode(activty_records_path)\n",
    "\n",
    "    # Add to the scraping log\n",
    "    write_csv(activty_records_path, mode_b, [\"link\"])\n",
    "\n",
    "    # Read the scraping log\n",
    "    df_backup = pd.read_csv(activty_records_path)\n",
    "\n",
    "    # Read the scraping log skipping duplicates\n",
    "    urls_to_skip = set(df_backup.link)\n",
    "\n",
    "    link = row[\"link\"]  # Gets the link from the 'link' column\n",
    "    title_link = row[\"title\"]  # Gets the text from the 'title' column\n",
    "    date_link = row[\"date\"]  # Gets the text from the 'date' column\n",
    "    src_url = row[\"page_url\"]  # Gets the text from the 'page_url' column\n",
    "\n",
    "    if link not in urls_to_skip:\n",
    "        try:\n",
    "            # Crawling with BeautifulSoup\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "                # Extract data from the website here\n",
    "                article_data = parse_unmadchedURLs(\n",
    "                    response, title_link, date_link, src_url\n",
    "                )\n",
    "\n",
    "                # Define field names\n",
    "                fieldnames = [\n",
    "                    \"title\",\n",
    "                    \"article_body\",\n",
    "                    \"story__summary\",\n",
    "                    \"detail_summary\",\n",
    "                    \"date\",\n",
    "                    \"link\",\n",
    "                    \"src_url\",\n",
    "                ]\n",
    "\n",
    "                # Crear directorio de salida si no existe\n",
    "                OUTPUT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"Output\"))\n",
    "                if not os.path.exists(OUTPUT_DIR):\n",
    "                    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "                # Nombre del archivo de salida\n",
    "                basename_output = os.path.basename(output_filename).split(\"_\")[0] + \"_output_notice_unmatchedURLs.csv\"\n",
    "                name_output = os.path.join(OUTPUT_DIR, basename_output)\n",
    "\n",
    "                # Check if output file exists\n",
    "                mode_a = check_mode(name_output)\n",
    "\n",
    "                # If the file exists, open in append mode, otherwise create (write mode)\n",
    "                write_csv(name_output, mode_a, fieldnames, article_data)\n",
    "\n",
    "                links_backup = {\"link\": link}\n",
    "\n",
    "                # Check if the file exists\n",
    "                mode_b = check_mode(activty_records_path)\n",
    "\n",
    "                # If the file exists, open in append mode\n",
    "                write_csv(activty_records_path, mode_b, ['link'], [links_backup])\n",
    "\n",
    "            elif response.status_code == 403:\n",
    "                print(\"Status is 403\")\n",
    "                print(\"Starting 3-minute wait\")\n",
    "\n",
    "                # Wait 3 minutes before retrying\n",
    "                for i in range(1, 4):\n",
    "                    time.sleep(60)\n",
    "                    print(f\"{i} minutes have passed\")\n",
    "                print(\"Restarting the scraper based on backup\")\n",
    "\n",
    "            elif response.status_code == 307:\n",
    "                print(link)\n",
    "                print(\"Too many redirects\")\n",
    "                pass  # Continue with the next iteration of the loop\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Check: \\n{link}')\n",
    "            print(f\"Error: {e}\")\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        print(f\"Row already processed, skipping...{activty_records_path}\")\n",
    "\n",
    "    print(\"step04_GETnotice_unmatchedURLs, Crawling complete\")\n",
    "\n",
    "def main_unmatched_URLs_loop(input_csv):\n",
    "    if os.path.exists(input_csv):\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            scraper_unmatchedURLS(row, input_csv)\n",
    "    else:\n",
    "        print(\"Input CSV file not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvlarepublica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
